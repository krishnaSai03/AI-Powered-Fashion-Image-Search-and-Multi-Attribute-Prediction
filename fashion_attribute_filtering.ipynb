{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76dcae39-441f-43a3-bf28-0b07ff1928c8",
   "metadata": {},
   "source": [
    "## AI-Powered Multi-Attribute Fashion Search and Prediction System\n",
    "### A two-phase approach to fashion image understanding using attribute-based filtering and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d367a8-be6a-4869-b7c9-b3c3cf50afdf",
   "metadata": {},
   "source": [
    "### 1. Environment Setup and Configuration\n",
    "This cell sets up the core environment and dependencies required for the project. It imports essential Python libraries for data handling (pandas, numpy), image manipulation (PIL), visualization (matplotlib), and interactive UI components (ipywidgets). It also includes PyTorch and HuggingFace Transformers for deep learning operations, specifically the CLIP model used for text-image embeddings.\n",
    "\n",
    "We then define the project base paths, including the root folder (BASE), the images directory (IM_DIR), and the optional captions file (CAP_FILE). This ensures that all files are consistently referenced throughout the project. Finally, the code detects whether a GPU (CUDA) is available for acceleration; if not, it falls back to CPU, and prints the device being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d131ba-946a-4bef-b58e-5f0d799b53ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, shutil, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import ipywidgets as W\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "BASE   = \"/Users/krishna/Desktop/FinalProject\"   \n",
    "IM_DIR = os.path.join(BASE, \"images\")           \n",
    "CAP_FILE = os.path.join(BASE, \"captions.json\")   \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed4ecb-527f-4bd6-8618-560605f354c3",
   "metadata": {},
   "source": [
    "### 2. Loading Attribute Annotations and Building Metadata Table\n",
    "In this step, we load the attribute annotations for each fashion image. The dataset provides three main types of labels:\n",
    "Shape Attributes – loaded from shape_anno_all.txt, containing 12 shape-related features (e.g., sleeves, pant length, neckline).\n",
    "\n",
    "Fabric Attributes – loaded from fabric_ann.txt, which records the fabric type for upper, lower, and outer garments.\n",
    "\n",
    "Color/Pattern Attributes – loaded from pattern_ann.txt, including upper, lower, and outer color or pattern information.\n",
    "\n",
    "Additionally, optional captions are loaded from captions.json (if available), giving natural language descriptions of the images. Each data source is converted into a Pandas DataFrame with meaningful column names, then merged into a single unified table (labels_df).\n",
    "\n",
    "To ensure data integrity, a helper function (find_path) verifies whether each image actually exists in the dataset folder. Only valid images with confirmed file paths are retained, and their absolute paths are stored in a new column img_path. This filtered dataset forms the master metadata table, linking every image to its shape, fabric, color, and caption attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecee3d2-5ef1-4e6b-93d7-91dee2a77b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images found: 44096\n"
     ]
    }
   ],
   "source": [
    "shape_df = pd.read_csv(os.path.join(BASE, 'labels/shape/shape_anno_all.txt'),\n",
    "                       sep=r'\\s+', header=None)\n",
    "shape_df.columns = ['image'] + [f'shape_{i}' for i in range(12)]\n",
    "\n",
    "fabric_df = pd.read_csv(os.path.join(BASE, 'labels/texture/fabric_ann.txt'),\n",
    "                        sep=r'\\s+', header=None, names=['image','upper_fabric','lower_fabric','outer_fabric'])\n",
    "\n",
    "pattern_df = pd.read_csv(os.path.join(BASE, 'labels/texture/pattern_ann.txt'),\n",
    "                         sep=r'\\s+', header=None, names=['image','upper_color','lower_color','outer_color'])\n",
    "\n",
    "# captions\n",
    "if os.path.exists(CAP_FILE):\n",
    "    with open(CAP_FILE) as f:\n",
    "        cap_json = json.load(f)\n",
    "    cap_df = pd.DataFrame(list(cap_json.items()), columns=['image','caption'])\n",
    "else:\n",
    "    cap_df = pd.DataFrame(columns=['image','caption'])\n",
    "\n",
    "labels_df = (shape_df.merge(fabric_df, on='image', how='outer')\n",
    "                      .merge(pattern_df, on='image', how='outer')\n",
    "                      .merge(cap_df, on='image', how='left'))\n",
    "\n",
    "# Build absolute image paths; keep only those that actually exist\n",
    "def find_path(img_name):\n",
    "    p = os.path.join(IM_DIR, img_name)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "labels_df[\"img_path\"] = labels_df[\"image\"].map(find_path)\n",
    "labels_df = labels_df[labels_df[\"img_path\"].notna()].reset_index(drop=True)\n",
    "print(\"Images found:\", len(labels_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f91a0-196b-43c0-9da4-31935a80e5b8",
   "metadata": {},
   "source": [
    "### 3. Mapping Numerical Annotations to Human-Readable Labels\n",
    "The raw dataset encodes all attributes (shapes, fabrics, and colors) as integer codes. To make these annotations interpretable, we define dictionaries that map numeric IDs to descriptive labels:\n",
    "\n",
    "shape_defs & shape_enums: Provide human-readable descriptions for shape-related attributes, such as sleeve length, hat presence, or neckline type.\n",
    "\n",
    "fabric_map: Translates fabric codes into materials like cotton, denim, or knitted.\n",
    "\n",
    "color_map: Maps pattern and color codes into categories like floral, striped, or pure color.\n",
    "Helper functions (safe_map and shape_text) are then used to safely transform these numeric columns into string labels, handling missing values gracefully by assigning \"NA\". New string columns (e.g., upper_fabric_str, upper_color_str, sleeves_str, hat_str) are created inside labels_df. This transformation ensures that attributes are human-readable for both analysis and the interactive UI, making the dataset much easier to explore and filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636e1bd8-f212-4299-964e-1e4b2eb5de37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>shape_0</th>\n",
       "      <th>shape_1</th>\n",
       "      <th>shape_2</th>\n",
       "      <th>shape_3</th>\n",
       "      <th>shape_4</th>\n",
       "      <th>shape_5</th>\n",
       "      <th>shape_6</th>\n",
       "      <th>shape_7</th>\n",
       "      <th>shape_8</th>\n",
       "      <th>...</th>\n",
       "      <th>upper_fabric_str</th>\n",
       "      <th>lower_fabric_str</th>\n",
       "      <th>outer_fabric_str</th>\n",
       "      <th>upper_color_str</th>\n",
       "      <th>lower_color_str</th>\n",
       "      <th>outer_color_str</th>\n",
       "      <th>sleeves_str</th>\n",
       "      <th>lowerlen_str</th>\n",
       "      <th>hat_str</th>\n",
       "      <th>glasses_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional.jpg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>cotton</td>\n",
       "      <td>cotton</td>\n",
       "      <td>NA</td>\n",
       "      <td>pure color</td>\n",
       "      <td>lattice</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>long</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>cotton</td>\n",
       "      <td>cotton</td>\n",
       "      <td>NA</td>\n",
       "      <td>pure color</td>\n",
       "      <td>pure color</td>\n",
       "      <td>NA</td>\n",
       "      <td>sleeveless</td>\n",
       "      <td>long</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image  shape_0  shape_1  shape_2  \\\n",
       "0  MEN-Denim-id_00000080-01_7_additional.jpg      5.0      3.0      0.0   \n",
       "1  MEN-Denim-id_00000089-01_7_additional.jpg      0.0      3.0      0.0   \n",
       "\n",
       "   shape_3  shape_4  shape_5  shape_6  shape_7  shape_8  ...  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      3.0  ...   \n",
       "1      0.0      0.0      0.0      0.0      0.0      3.0  ...   \n",
       "\n",
       "   upper_fabric_str  lower_fabric_str  outer_fabric_str  upper_color_str  \\\n",
       "0            cotton            cotton                NA       pure color   \n",
       "1            cotton            cotton                NA       pure color   \n",
       "\n",
       "   lower_color_str  outer_color_str  sleeves_str  lowerlen_str  hat_str  \\\n",
       "0          lattice               NA           NA          long       no   \n",
       "1       pure color               NA   sleeveless          long       no   \n",
       "\n",
       "  glasses_str  \n",
       "0          no  \n",
       "1          no  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Maps\n",
    "shape_defs = {\n",
    "    0:'sleeve length',1:'lower clothing length',2:'socks',3:'hat',4:'glasses',5:'neckwear',\n",
    "    6:'wrist wearing',7:'ring',8:'waist accessories',9:'neckline',10:'outer clothing a cardigan?',11:'upper clothing covering navel'\n",
    "}\n",
    "shape_enums = {\n",
    "    0: {0:'sleeveless',1:'short',2:'medium',3:'long',4:'not long-sleeve',5:'NA'},\n",
    "    1: {0:'three-point',1:'medium short',2:'three-quarter',3:'long',4:'NA'},\n",
    "    2: {0:'no',1:'socks',2:'leggings',3:'NA'},\n",
    "    3: {0:'no',1:'yes',2:'NA'},\n",
    "    4: {0:'no',1:'eyeglasses',2:'sunglasses',3:'have a glasses in hand or clothes',4:'NA'},\n",
    "    5: {0:'no',1:'yes',2:'NA'},\n",
    "    6: {0:'no',1:'yes',2:'NA'},\n",
    "    7: {0:'no',1:'yes',2:'NA'},\n",
    "    8: {0:'no',1:'belt',2:'have a clothing',3:'hidden',4:'NA'},\n",
    "    9: {0:'V-shape',1:'square',2:'round',3:'standing',4:'lapel',5:'suspenders',6:'NA'},\n",
    "    10:{0:'yes',1:'no',2:'NA'},\n",
    "    11:{0:'no',1:'yes',2:'NA'}\n",
    "}\n",
    "fabric_map = {0:'denim',1:'cotton',2:'leather',3:'furry',4:'knitted',5:'chiffon',6:'other',7:'NA'}\n",
    "color_map  = {0:'floral',1:'graphic',2:'striped',3:'pure color',4:'lattice',5:'other',6:'color block',7:'NA'}\n",
    "\n",
    "def safe_map(series, dct, prefix):\n",
    "    \"\"\"Map integer codes to text; fallback to prefix_idx if map unknown.\"\"\"\n",
    "    def _one(v):\n",
    "        if pd.isna(v): return \"NA\"\n",
    "        try:\n",
    "            vi = int(v)\n",
    "            return dct.get(vi, f\"{prefix}_{vi}\") if dct else f\"{prefix}_{vi}\"\n",
    "        except Exception:\n",
    "            return \"NA\"\n",
    "    return series.map(_one)\n",
    "\n",
    "# Fabric/color string columns\n",
    "try:\n",
    "    labels_df[\"upper_fabric_str\"] = safe_map(labels_df[\"upper_fabric\"], globals().get(\"fabric_map\"), \"fabric\")\n",
    "    labels_df[\"lower_fabric_str\"] = safe_map(labels_df[\"lower_fabric\"], globals().get(\"fabric_map\"), \"fabric\")\n",
    "    labels_df[\"outer_fabric_str\"] = safe_map(labels_df[\"outer_fabric\"], globals().get(\"fabric_map\"), \"fabric\")\n",
    "    labels_df[\"upper_color_str\"]  = safe_map(labels_df[\"upper_color\"],  globals().get(\"color_map\"),  \"color\")\n",
    "    labels_df[\"lower_color_str\"]  = safe_map(labels_df[\"lower_color\"],  globals().get(\"color_map\"),  \"color\")\n",
    "    labels_df[\"outer_color_str\"]  = safe_map(labels_df[\"outer_color\"],  globals().get(\"color_map\"),  \"color\")\n",
    "except Exception as e:\n",
    "    print(\"Mapping error:\", e)\n",
    "\n",
    "# Shape strings (sleeves, lower length, hat, glasses)\n",
    "def shape_text(col_idx, series, prefix):\n",
    "    enum = None if \"shape_enums\" not in globals() else shape_enums.get(col_idx)\n",
    "    def _one(v):\n",
    "        if pd.isna(v): return \"NA\"\n",
    "        try:\n",
    "            vi = int(v)\n",
    "            if enum: return enum.get(vi, f\"{prefix}_{vi}\")\n",
    "            return f\"{prefix}_{vi}\"\n",
    "        except Exception:\n",
    "            return \"NA\"\n",
    "    return series.map(_one)\n",
    "\n",
    "labels_df[\"sleeves_str\"]   = shape_text(0, labels_df[\"shape_0\"], \"sleeves\")\n",
    "labels_df[\"lowerlen_str\"]  = shape_text(1, labels_df[\"shape_1\"], \"lowerlen\")\n",
    "labels_df[\"hat_str\"]       = shape_text(3, labels_df[\"shape_3\"], \"hat\")\n",
    "labels_df[\"glasses_str\"]   = shape_text(4, labels_df[\"shape_4\"], \"glasses\")\n",
    "\n",
    "labels_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc92fca-6518-4b8c-8726-1df77f5556bb",
   "metadata": {},
   "source": [
    "### 4. Loading Precomputed CLIP Embeddings from Cache\n",
    "This step handles the loading of CLIP image embeddings, which were precomputed earlier to save time. Instead of recalculating embeddings for every run, the system reads them from a cached file stored in .cache/clip_vitb32_img_embeds.npz. This file contains:\n",
    "\n",
    "##### paths – the list of image file paths corresponding to each embedding.\n",
    "\n",
    "##### embeddings – the actual CLIP vector representations of the images.\n",
    "\n",
    "A sanity check ensures that the cached image paths exactly match the current labels_df[\"img_path\"] order. If there’s any mismatch (e.g., if dataset files were moved or renamed), the code raises a clear error and instructs the user to rebuild the cache.\n",
    "\n",
    "Finally, the embeddings are L2-normalized, so cosine similarity can be computed directly as a dot product. The output prints the total shape of the embedding matrix, confirming how many images are represented and the dimensionality of each vector. This caching mechanism significantly improves performance, as embeddings only need to be computed once and can then be reused for fast similarity search and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e53dea8-a92b-4a0a-8e80-942a7eb818d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (44096, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "\n",
    "CACHE_DIR = os.path.join(BASE, \".cache\")\n",
    "EMB_NPZ   = os.path.join(CACHE_DIR, \"clip_vitb32_img_embeds.npz\")\n",
    "\n",
    "if not os.path.exists(EMB_NPZ):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing cache: {EMB_NPZ}\\n\"\n",
    "        \"Run the original Step 4 once to build the embeddings cache.\"\n",
    "    )\n",
    "\n",
    "cache = np.load(EMB_NPZ, allow_pickle=True)\n",
    "cached_paths = cache[\"paths\"].tolist()\n",
    "img_embs     = cache[\"embeddings\"]\n",
    "\n",
    "# sanity check: ensure cached paths match current labels_df order\n",
    "if cached_paths != labels_df[\"img_path\"].tolist():\n",
    "    raise RuntimeError(\n",
    "        \"Cached image list differs from current labels_df. \"\n",
    "        \"Since you don't add images, this likely means paths changed; \"\n",
    "        \"restore the old paths or rebuild the cache once.\"\n",
    "    )\n",
    "\n",
    "# normalize (just to be safe)\n",
    "img_embs = img_embs / (np.linalg.norm(img_embs, axis=1, keepdims=True) + 1e-9)\n",
    "print(\"Loaded embeddings:\", img_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52be958d-93c1-4d32-9c23-b14a682a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d890cb-cca4-4355-9670-2adbf5e5208d",
   "metadata": {},
   "source": [
    "### 5. Initializing FAISS Index and CLIP Text Encoder\n",
    "##### This section sets up the search engine for content-based retrieval:\n",
    "\n",
    "FAISS Integration: The code first attempts to import FAISS, a high-performance similarity search library. If available, it builds an IndexFlatIP (inner-product index) using the preloaded CLIP embeddings, enabling very fast nearest-neighbor searches. If FAISS is not available, the system falls back to a slower NumPy-based dot product approach for cosine similarity. This ensures portability across environments.\n",
    "\n",
    "CLIP Text Encoder Setup: Next, the CLIP text encoder (CLIPModel and CLIPProcessor) is re-initialized to guarantee availability. The model is loaded onto GPU if available (cuda), otherwise defaults to CPU. This encoder transforms free-text queries into vector embeddings aligned with the image embeddings.\n",
    "\n",
    "##### Utility Functions:\n",
    "text_to_vec(query) → Converts a query string into a normalized CLIP text embedding. cosine_search_text(query, topn) → Retrieves the top-n most similar images for a given query. If FAISS is enabled, it uses fast indexing; otherwise, it computes cosine similarity directly with NumPy. This dual setup provides the core retrieval mechanism: CLIP ensures semantic alignment between text and images, while FAISS accelerates the similarity search, making the interactive filtering system responsive and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8023473e-a318-4d25-95bb-1c0d38c9e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import faiss\n",
    "    use_faiss = True\n",
    "except Exception:\n",
    "    use_faiss = False\n",
    "    print(\"FAISS not available → falling back to NumPy search.\")\n",
    "\n",
    "if use_faiss:\n",
    "    index = faiss.IndexFlatIP(img_embs.shape[1])\n",
    "    index.add(img_embs.astype(\"float32\"))\n",
    "# ---- CLIP init (text encoder) — tiny patch ----\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# (Re)create CLIP objects if missing\n",
    "if 'clip_model' not in globals() or clip_model is None:\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "\n",
    "if 'clip_proc' not in globals() or clip_proc is None:\n",
    "    clip_proc  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def text_to_vec(query: str) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_proc(text=[query], return_tensors=\"pt\", padding=True).to(device)\n",
    "        t = clip_model.get_text_features(**inputs)            # (1, 512)\n",
    "        t = torch.nn.functional.normalize(t, dim=-1)\n",
    "    return t.cpu().numpy()[0]\n",
    "\n",
    "def cosine_search_text(query: str, topn=400):\n",
    "    q = text_to_vec(query)\n",
    "    if use_faiss:\n",
    "        sims, idx = index.search(q.astype(\"float32\")[None, :], topn)\n",
    "        return sims[0], idx[0]\n",
    "    else:\n",
    "        sims = img_embs @ q\n",
    "        idx = np.argsort(-sims)[:topn]\n",
    "        return sims[idx], idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b546a9-7226-4305-9b5b-86a06517e436",
   "metadata": {},
   "source": [
    "### 6. Attribute Filtering and Scoring Utilities\n",
    "This block defines the tools for filtering search results by user-selected attributes and ranking them based on combined similarity and attribute match scores:\n",
    "\n",
    "Dropdown Options (opts): For each attribute column (e.g., sleeves, hat, fabrics, colors), this function extracts the unique values from the dataset, cleans them up, and provides the list of valid choices for the Gradio dropdown menus. If no values exist, it defaults to a placeholder.\n",
    "\n",
    "Attribute Columns (ATTR_COLS): A structured list of all attributes that the user can filter on, mapped to human-readable names for UI display.\n",
    "\n",
    "#### Filtering Functions:\n",
    "apply_attr_mask(df, selections) applies Boolean masks to the candidate dataset, keeping only rows that match the selected attributes.\n",
    "\n",
    "attr_match_score(row, selections) calculates how well a given image matches the chosen attributes, producing a fractional score (0–1). For example, if 4 attributes are chosen and 3 match, the score is 0.75.\n",
    "\n",
    "Result Formatting (format_caption): Creates a structured, human-readable caption for each image. It displays the decoded attribute labels (sleeves, fabrics, colors, etc.), along with the cosine similarity score, the attribute match score, and the final blended score.\n",
    "\n",
    "Together, these utilities enable the system to not only find visually similar images but also respect user-defined attribute filters, ensuring precise and interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df47ee8-ab60-43ba-b2ef-35852f2281b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opts(col):\n",
    "    vals = sorted([str(v) for v in labels_df[col].dropna().unique().tolist() if str(v) != \"NA\"])\n",
    "    return vals if vals else [\"(none)\"]\n",
    "\n",
    "ATTR_COLS = [\n",
    "    (\"sleeves_str\",      \"Sleeves\"),\n",
    "    (\"lowerlen_str\",     \"Lower Length\"),\n",
    "    (\"hat_str\",          \"Hat\"),\n",
    "    (\"glasses_str\",      \"Glasses\"),\n",
    "    (\"upper_fabric_str\", \"Upper Fabric\"),\n",
    "    (\"lower_fabric_str\", \"Lower Fabric\"),\n",
    "    (\"outer_fabric_str\", \"Outer Fabric\"),\n",
    "    (\"upper_color_str\",  \"Upper Color\"),\n",
    "    (\"lower_color_str\",  \"Lower Color\"),\n",
    "    (\"outer_color_str\",  \"Outer Color\"),\n",
    "]\n",
    "def search_pipeline(query, alpha, top_k, \n",
    "                    sleeves, lowerlen, hat, glasses,\n",
    "                    upfab, lofab, oufab, upcol, locol, oucol):\n",
    "    top_k = int(top_k)  # <-- add this line\n",
    "    \n",
    "def apply_attr_mask(df, selections: dict):\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for col, _label in ATTR_COLS:\n",
    "        chosen = selections.get(col, [])\n",
    "        if chosen:  # apply only if user selected values\n",
    "            mask &= df[col].isin(chosen)\n",
    "    return df[mask]\n",
    "\n",
    "def attr_match_score(row, selections: dict):\n",
    "    wanted = [(c, set(v)) for c,v in selections.items() if v]\n",
    "    if not wanted: return 1.0\n",
    "    hits = 0\n",
    "    for c, choices in wanted:\n",
    "        val = row.get(c, None)\n",
    "        if pd.isna(val): continue\n",
    "        if val in choices: hits += 1\n",
    "    return hits / len(wanted)\n",
    "\n",
    "def format_caption(row, cos=0.0, attr=0.0, final=0.0):\n",
    "    return (\n",
    "        f\"{row['image']}\\n\"\n",
    "        f\"Sleeves:{row.get('sleeves_str','NA')}  Lower:{row.get('lowerlen_str','NA')}\\n\"\n",
    "        f\"UpFab:{row.get('upper_fabric_str','NA')}  LoFab:{row.get('lower_fabric_str','NA')}  OutFab:{row.get('outer_fabric_str','NA')}\\n\"\n",
    "        f\"UpCol:{row.get('upper_color_str','NA')}  LoCol:{row.get('lower_color_str','NA')}  OutCol:{row.get('outer_color_str','NA')}\\n\"\n",
    "        f\"Hat:{row.get('hat_str','NA')}  Glasses:{row.get('glasses_str','NA')}\\n\"\n",
    "        f\"cos={cos:.2f}  attr={attr:.2f}  score={final:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165a9e6-a702-436f-9bc8-2c05d461f92e",
   "metadata": {},
   "source": [
    "### 7. End-to-End Search Pipeline Implementation\n",
    "This block ties together content-based similarity and multi-attribute filtering into one unified search function for the interactive tool:\n",
    "Safe Wrapper (search_pipeline_safe): Runs the main pipeline inside a try/except block. If an error occurs (e.g., missing data, unexpected input), it gracefully returns an empty gallery and shows a readable error message in the Gradio UI instead of crashing the app.\n",
    "\n",
    "##### Main Search Pipeline (search_pipeline):\n",
    "\n",
    "Query Validation – Ensures that the user has provided a non-empty text query.\n",
    "\n",
    "Content Search – Uses CLIP embeddings to retrieve candidate images most similar to the text query. The top candidates are returned with cosine similarity scores.\n",
    "\n",
    "Attribute Selections – Collects the user’s filter choices (sleeves, fabric, colors, etc.) into a dictionary.\n",
    "\n",
    "Filtering + Scoring – Applies attribute masks to eliminate non-matching items. Then, for each remaining image, calculates an attribute match score and blends it with the cosine similarity using a tunable weight parameter α:\n",
    "\n",
    "\n",
    "final_score=α⋅cosine_sim+(1−α)⋅attr_score\n",
    "This allows balancing between visual similarity and attribute strictness.\n",
    "\n",
    "##### Ranking – Sorts results by final score, selects the top k, and prepares them for display.\n",
    "\n",
    "##### Output – Returns each result as an (image path, caption) pair, where captions include attributes and scores.\n",
    "\n",
    "The output also includes a status message (e.g., “Showing 24 results”), giving the user clear feedback about the search results. This function is the heart of the Phase-1 system, combining content embeddings, FAISS search, and rule-based attribute filtering into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c411d98-8bf6-4805-94e0-b80d2fed3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def search_pipeline_safe(*args, **kwargs):\n",
    "    try:\n",
    "        items, status = search_pipeline(*args, **kwargs)\n",
    "        # Gradio sometimes prefers tuples; convert to tuples to be safe\n",
    "        items = [(p, c) for (p, c) in items]\n",
    "        return items, status\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        # Return empty gallery and show the error inside the app\n",
    "        return [], f\"**Error in search:** {e}\\n\\n```\\n{tb}\\n```\"\n",
    "\n",
    "def search_pipeline(query, alpha, top_k,\n",
    "                    sleeves, lowerlen, hat, glasses,\n",
    "                    upfab, lofab, oufab,\n",
    "                    upcol, locol, oucol):\n",
    "    if not query or not query.strip():\n",
    "        return [], \"Please enter a text query.\"\n",
    "\n",
    "    # 1) content candidates\n",
    "    sims, idxs = cosine_search_text(query.strip(), topn=max(200, top_k*6))\n",
    "    cand = labels_df.iloc[idxs].copy()\n",
    "    cand[\"cosine_sim\"] = sims\n",
    "\n",
    "    # 2) attribute selections\n",
    "    selections = {\n",
    "        \"sleeves_str\": sleeves or [],\n",
    "        \"lowerlen_str\": lowerlen or [],\n",
    "        \"hat_str\": hat or [],\n",
    "        \"glasses_str\": glasses or [],\n",
    "        \"upper_fabric_str\": upfab or [],\n",
    "        \"lower_fabric_str\": lofab or [],\n",
    "        \"outer_fabric_str\": oufab or [],\n",
    "        \"upper_color_str\": upcol or [],\n",
    "        \"lower_color_str\": locol or [],\n",
    "        \"outer_color_str\": oucol or [],\n",
    "    }\n",
    "\n",
    "    # 3) filter + score\n",
    "    cand = apply_attr_mask(cand, selections).copy()\n",
    "    if len(cand) == 0:\n",
    "        return [], \"No matches for this combination. Try relaxing filters.\"\n",
    "\n",
    "    cand[\"attr_score\"] = cand.apply(lambda r: attr_match_score(r, selections), axis=1)\n",
    "    cand[\"final_score\"] = float(alpha)*cand[\"cosine_sim\"] + (1-float(alpha))*cand[\"attr_score\"]\n",
    "    cand = cand.sort_values(\"final_score\", ascending=False).head(int(top_k)).reset_index(drop=True)\n",
    "\n",
    "    # 4) return gallery items: (image, caption)\n",
    "    items = []\n",
    "    for _, r in cand.iterrows():\n",
    "        cap = format_caption(r, r[\"cosine_sim\"], r[\"attr_score\"], r[\"final_score\"])\n",
    "        items.append([r[\"img_path\"], cap])\n",
    "    status = f\"Showing {len(items)} result(s).\"\n",
    "    return items, status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038fac1c-419e-4020-b80a-4988a3fdd02b",
   "metadata": {},
   "source": [
    "### 8. Building the Interactive Gradio Interface\n",
    "This cell creates the front-end user interface for Phase-1, enabling interactive content-based fashion search with multi-attribute filters:\n",
    "\n",
    "Query Controls:\n",
    "\n",
    "Text Query Box – Users can enter a natural language description like “red floral dress with long sleeves” to trigger CLIP-based similarity search.\n",
    "\n",
    "Content Weight Slider (α) – Lets users control the balance between visual similarity (cosine score) and attribute matching.\n",
    "\n",
    "Top K Slider – Adjusts how many top-ranked results are displayed in the gallery.\n",
    "\n",
    "Attribute Filters: Multi-select dropdown menus are generated dynamically from the dataset using opts_map. Users can refine results by choosing specific shape attributes (sleeves, hat, glasses), fabrics (upper, lower, outer), and colors (upper, lower, outer). These filters directly connect to the backend pipeline.\n",
    "\n",
    "Results Display:\n",
    "\n",
    "Search Button executes the search_pipeline_safe function, passing the query, weight, and filter selections.\n",
    "\n",
    "Gallery Component shows the retrieved images in a clean 4-column layout, along with captions containing attribute values and scores.\n",
    "\n",
    "Status Widget displays feedback, such as the number of results found or any caught errors.\n",
    "\n",
    "Execution: Finally, demo.launch() starts a lightweight local web app, allowing the user to run real-time searches outside the notebook.\n",
    "\n",
    "This interface integrates the backend pipeline with a user-friendly front end, enabling intuitive exploration of the dataset while combining textual search and structured attribute filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f3e8df-e360-486b-866c-9902e910a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Precompute options for dropdowns\n",
    "opts_map = {col: opts(col) for col, _ in ATTR_COLS}\n",
    "\n",
    "with gr.Blocks() as demo:  # no title arg for compatibility\n",
    "    gr.Markdown(\"## Content-Based Search with Multi-Attribute Filters\")\n",
    "\n",
    "    with gr.Row():\n",
    "        query = gr.Textbox(label=\"Text Query\", placeholder=\"e.g., red floral dress with long sleeves\")\n",
    "        alpha = gr.Slider(0.0, 1.0, value=0.8, step=0.05, label=\"Content weight (α)\")\n",
    "        topk  = gr.Slider(6, 60, value=24, step=6, label=\"Top K\")\n",
    "\n",
    "    with gr.Row():\n",
    "        sleeves   = gr.Dropdown(opts_map[\"sleeves_str\"],  multiselect=True, label=\"Sleeves\")\n",
    "        lowerlen  = gr.Dropdown(opts_map[\"lowerlen_str\"], multiselect=True, label=\"Lower Length\")\n",
    "        hat       = gr.Dropdown(opts_map[\"hat_str\"],      multiselect=True, label=\"Hat\")\n",
    "        glasses   = gr.Dropdown(opts_map[\"glasses_str\"],  multiselect=True, label=\"Glasses\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upfab = gr.Dropdown(opts_map[\"upper_fabric_str\"], multiselect=True, label=\"Upper Fabric\")\n",
    "        lofab = gr.Dropdown(opts_map[\"lower_fabric_str\"], multiselect=True, label=\"Lower Fabric\")\n",
    "        oufab = gr.Dropdown(opts_map[\"outer_fabric_str\"], multiselect=True, label=\"Outer Fabric\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upcol = gr.Dropdown(opts_map[\"upper_color_str\"], multiselect=True, label=\"Upper Color\")\n",
    "        locol = gr.Dropdown(opts_map[\"lower_color_str\"], multiselect=True, label=\"Lower Color\")\n",
    "        oucol = gr.Dropdown(opts_map[\"outer_color_str\"], multiselect=True, label=\"Outer Color\")\n",
    "\n",
    "    run_btn = gr.Button(\"Search\")\n",
    "\n",
    "    # Gallery: v3/v4 friendly settings\n",
    "    gallery = gr.Gallery(\n",
    "        label=\"Results\",\n",
    "        show_label=True,\n",
    "        columns=4,\n",
    "        height=800,\n",
    "        allow_preview=True,     # ok on v3/4\n",
    "        object_fit=\"contain\"    # better rendering; safe on v3/4\n",
    "    )\n",
    "    status  = gr.Markdown()\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=search_pipeline_safe,\n",
    "        inputs=[query, alpha, topk,\n",
    "                sleeves, lowerlen, hat, glasses,\n",
    "                upfab, lofab, oufab,\n",
    "                upcol, locol, oucol],\n",
    "        outputs=[gallery, status]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
